include {
  UnGzip
 } from '../workflows/classify'

workflow nanoclust {
    take:
        sample_reads // Channel<Map, Fastq.GZ> one per repetition, per sample

    main:
        min_cluster_props = Channel.fromList(params.cluster.nanoclust.min_cluster_props)

        umap = UnGzip(sample_reads)
            | kmerFreq
            | umapTransform

//         min_cluster_props.combine(umap.umap_tsv)
//             .map { min_cluster_prop, meta, umap_tsv -> [meta + [ min_cluster_prop: min_cluster_prop ], umap_tsv ] }
//             | hdbscanClustering

        plotMinClusterSizeEffect(umap.umap_tsv)

}

process kmerFreq {
    container "docker.io/hecrp/nanoclust-kmer_freqs:latest"

    input:
        tuple val(meta), path(fastq)
    output:
        tuple val(meta), path("*.tsv"), emit: kmer_freqs

    script:
    """
    kmer_freq.py -t $task.cpus \\
        -k 6 \\
        -r $fastq > freqs.tsv
    """
}

process umapTransform {
    container "docker.io/hecrp/nanoclust-read_clustering:latest"

    input:
        tuple val(meta), path(kmer_freqs)
    output:
        tuple val(meta), path("*output.tsv"), emit: umap_tsv

    script:
    template "umap_transform.py"
}

process plotMinClusterSizeEffect {
    container "docker.io/hecrp/nanoclust-read_clustering:latest"

    input:
        tuple val(meta), path(umap_tsv)
    output:
        tuple val(meta), path("*.png"), emit: png

    script:
    """
        #!/usr/bin/env python

        import numpy as np
        import matplotlib.pyplot as plt
        import pandas as pd
        import hdbscan

        umap_out = pd.read_csv("$umap_tsv", delimiter="\t")

        X = umap_out.loc[:, ["D1", "D2"]]

        props = np.linspace(0.00001, 0.1, 201)
        y = np.zeros(len(props))

        for i in range(0, len(props)):
            min_cluster_size = max(2, int(len(umap_out)*props[i]))
            preds = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, cluster_selection_epsilon=0.5).fit_predict(X)
            y[i] = len(np.unique(preds)) - 1

        fig, ax = plt.subplots()
        ax.plot(props, y)
        plt.xlabel("min cluster size (as proportion of total library size)", fontsize=18)
        plt.ylabel("number of clusters", fontsize=18)
        plt.savefig('clusters-by-size.png')
    """
}

process hdbscanClustering {
    container "docker.io/hecrp/nanoclust-read_clustering:latest"

    input:
        tuple val(meta), path(umap_tsv)
    output:
        tuple val(meta), path("*.tsv"), emit: hdbscan
        tuple val(meta), path("*.png"), emit: png

    script:
    def min_cluster_prop=meta.min_cluster_prop
    def cluster_sel_epsilon=0.5
    """
    #!/usr/bin/env python

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    import hdbscan

    umap_out = pd.read_csv("$umap_tsv", delimiter="\t")

    X = umap_out.loc[:, ["D1", "D2"]]

    min_cluster_size = max(2, int(len(umap_out)*$min_cluster_prop))
    umap_out["bin_id"] = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, cluster_selection_epsilon=0.5).fit_predict(X)

    #PLOT
    plt.figure(figsize=(20,20))
    plt.scatter(umap_out.loc[:, "D1"], umap_out.loc[:, "D2"], c=umap_out["bin_id"], cmap='Spectral', s=1)
    plt.xlabel("UMAP1", fontsize=18)
    plt.ylabel("UMAP2", fontsize=18)
    plt.gca().set_aspect('equal', 'datalim')
    plt.title("Projecting " + str(len(umap_out['bin_id'])) + " reads. " + str(len(umap_out['bin_id'].unique())) + " clusters generated by HDBSCAN", fontsize=18)

    for cluster in np.sort(umap_out['bin_id'].unique()):
        read = umap_out.loc[umap_out['bin_id'] == cluster].iloc[0]
        plt.annotate(str(cluster), (read['D1'], read['D2']), weight='bold', size=14)

    plt.savefig(f'hdbscan.output-{min_cluster_size}.png')
    umap_out.loc[:, ["read", "bin_id"]].to_csv(f"hdbscan.output-{min_cluster_size}.tsv", sep="\t", index=False)
    """
}